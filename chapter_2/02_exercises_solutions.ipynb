{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 2 â€“ End-to-end Machine Learning project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook contains all the sample code and solutions to the exercises in chapter 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Try a Support Vector Machine regressor (`sklearn.svm.SVR`) with various hyperparameters, such as `kernel=\"linear\"` (with various values for the `C` hyperparameter) or `kernel=\"rbf\"` (with various values for the `C` and `gamma` hyperparameters). Note that SVMs don't scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don't worry about what the hyperparameters mean for now (see the SVM notebook if you're interested). How does the best `SVR` predictor perform?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
       "                                                                                     SimpleImputer(strategy='median')),\n",
       "                                                                                    ('standardscaler',\n",
       "                                                                                     StandardScaler())]),\n",
       "                                                          transformers=[('bedrooms',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('functiontransformer',\n",
       "                                                                                          FunctionTransformer(feature_names_out=<f...\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),\n",
       "                                       ('svr', SVR())]),\n",
       "             param_grid=[{'svr__C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0,\n",
       "                                     10000.0, 30000.0],\n",
       "                          'svr__kernel': ['linear']},\n",
       "                         {'svr__C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0,\n",
       "                                     1000.0],\n",
       "                          'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
       "                          'svr__kernel': ['rbf']}],\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = [\n",
    "        {'svr__kernel': ['linear'], 'svr__C': [10., 30., 100., 300., 1000.,\n",
    "                                               3000., 10000., 30000.0]},\n",
    "        {'svr__kernel': ['rbf'], 'svr__C': [1.0, 3.0, 10., 30., 100., 300.,\n",
    "                                            1000.0],\n",
    "         'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svr_pipeline = Pipeline([(\"preprocessing\", preprocessing), (\"svr\", SVR())])\n",
    "grid_search = GridSearchCV(svr_pipeline, param_grid, cv=3,\n",
    "                           scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model achieves the following score (evaluated using 3-fold cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69814.13889867254"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_grid_search_rmse = -grid_search.best_score_\n",
    "svr_grid_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much worse than the `RandomForestRegressor` (but to be fair, we trained the model on much less data). Let's check the best hyperparameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svr__C': 10000.0, 'svr__kernel': 'linear'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel seems better than the RBF kernel. Notice that the value of `C` is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for `C` (removing the smallest values), because it is likely that higher values of `C` will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Try replacing the `GridSearchCV` with a `RandomizedSearchCV`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** the following cell will take several minutes to run. You can specify `verbose=2` when creating the `RandomizedSearchCV` if you want to see the training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessing',\n",
       "                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
       "                                                                                           SimpleImputer(strategy='median')),\n",
       "                                                                                          ('standardscaler',\n",
       "                                                                                           StandardScaler())]),\n",
       "                                                                transformers=[('bedrooms',\n",
       "                                                                               Pipeline(steps=[('simpleimputer',\n",
       "                                                                                                SimpleImputer(strategy='median')),\n",
       "                                                                                               ('functiontransformer',\n",
       "                                                                                                FunctionTransformer(feature_names_...\n",
       "                                                                               <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),\n",
       "                                             ('svr', SVR())]),\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a5d4c3a0>,\n",
       "                                        'svr__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a5d9ca00>,\n",
       "                                        'svr__kernel': ['linear', 'rbf']},\n",
       "                   random_state=42, scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, loguniform\n",
    "\n",
    "# see https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "# for `expon()` and `loguniform()` documentation and more probability distribution functions.\n",
    "\n",
    "# Note: gamma is ignored when kernel is \"linear\"\n",
    "param_distribs = {\n",
    "        'svr__kernel': ['linear', 'rbf'],\n",
    "        'svr__C': loguniform(20, 200_000),\n",
    "        'svr__gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "rnd_search = RandomizedSearchCV(svr_pipeline,\n",
    "                                param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=3,\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                random_state=42)\n",
    "rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model achieves the following score (evaluated using 3-fold cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55853.88100300133"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rnd_search_rmse = -rnd_search.best_score_\n",
    "svr_rnd_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's really much better, but still far from the `RandomForestRegressor`'s performance. Let's check the best hyperparameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svr__C': 157055.10989448498,\n",
       " 'svr__gamma': 0.26497040005002437,\n",
       " 'svr__kernel': 'rbf'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used the `expon()` distribution for `gamma`, with a scale of 1, so `RandomSearch` mostly searched for values roughly of that scale: about 80% of the samples were between 0.1 and 2.3 (roughly 10% were smaller and 10% were larger):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80066"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "s = expon(scale=1).rvs(100_000)  # get 100,000 samples\n",
    "((s > 0.105) & (s < 2.29)).sum() / 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `loguniform()` distribution for `C`, meaning we did not have a clue what the optimal scale of `C` was before running the random search. It explored the range from 20 to 200 just as much as the range from 2,000 to 20,000 or from 20,000 to 200,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Try adding a `SelectFromModel` transformer in the preparation pipeline to select only the most important attributes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new pipeline that runs the previously defined preparation pipeline, and adds a `SelectFromModel` transformer based on a `RandomForestRegressor` before the final regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),\n",
    "                                 threshold=0.005)),  # min feature importance\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "                gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "                kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        3.000000\n",
       "mean     56211.362086\n",
       "std       1922.002802\n",
       "min      54150.008629\n",
       "25%      55339.929909\n",
       "50%      56529.851189\n",
       "75%      57242.038815\n",
       "max      57954.226441\n",
       "dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_rmses = -cross_val_score(selector_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(selector_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well, feature selection does not seem to help. But maybe that's just because the threshold we used was not optimal. Perhaps try tuning it using random search or grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Try creating a custom transformer that trains a k-Nearest Neighbors regressor (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the model's predictions in its `transform()` method. Then add this feature to the preprocessing pipeline, using latitude and longitude as the inputs to this transformer. This will add a feature in the model that corresponds to the housing median price of the nearest districts._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than restrict ourselves to k-Nearest Neighbors regressors, let's create a transformer that accepts any regressor. For this, we can extend the `MetaEstimatorMixin` and have a required `estimator` argument in the constructor. The `fit()` method must work on a clone of this estimator, and it must also save `feature_names_in_`. The `MetaEstimatorMixin` will ensure that `estimator` is listed as a required parameters, and it will update `get_params()` and `set_params()` to make the estimator's hyperparameters available for tuning. Lastly, we create a `get_feature_names_out()` method: the output column name is the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import MetaEstimatorMixin, clone\n",
    "\n",
    "class FeatureFromRegressor(MetaEstimatorMixin, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        estimator_ = clone(self.estimator)\n",
    "        estimator_.fit(X, y)\n",
    "        self.estimator_ = estimator_\n",
    "        self.n_features_in_ = self.estimator_.n_features_in_\n",
    "        if hasattr(self.estimator, \"feature_names_in_\"):\n",
    "            self.feature_names_in_ = self.estimator.feature_names_in_\n",
    "        return self  # always return self!\n",
    "    \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        predictions = self.estimator_.predict(X)\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        return predictions\n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        check_is_fitted(self)\n",
    "        n_outputs = getattr(self.estimator_, \"n_outputs_\", 1)\n",
    "        estimator_class_name = self.estimator_.__class__.__name__\n",
    "        estimator_short_name = estimator_class_name.lower().replace(\"_\", \"\")\n",
    "        return [f\"{estimator_short_name}_prediction_{i}\"\n",
    "                for i in range(n_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure it complies to Scikit-Learn's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(FeatureFromRegressor(KNeighborsRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[456667.        ],\n",
       "       [435250.        ],\n",
       "       [105100.        ],\n",
       "       ...,\n",
       "       [148800.        ],\n",
       "       [500001.        ],\n",
       "       [234333.33333333]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_reg = KNeighborsRegressor(n_neighbors=3, weights=\"distance\")\n",
    "knn_transformer = FeatureFromRegressor(knn_reg)\n",
    "geo_features = housing[[\"latitude\", \"longitude\"]]\n",
    "knn_transformer.fit_transform(geo_features, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what does its output feature name look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kneighborsregressor_prediction_0']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's include this transformer in our preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "transformers = [(name, clone(transformer), columns)\n",
    "                for name, transformer, columns in preprocessing.transformers]\n",
    "geo_index = [name for name, _, _ in transformers].index(\"geo\")\n",
    "transformers[geo_index] = (\"geo\", knn_transformer, [\"latitude\", \"longitude\"])\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_geo_pipeline = Pipeline([\n",
    "    ('preprocessing', new_geo_preprocessing),\n",
    "    ('svr', SVR(C=rnd_search.best_params_[\"svr__C\"],\n",
    "                gamma=rnd_search.best_params_[\"svr__gamma\"],\n",
    "                kernel=rnd_search.best_params_[\"svr__kernel\"])),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         3.000000\n",
       "mean     104992.095758\n",
       "std        3112.486560\n",
       "min      101550.880533\n",
       "25%      103682.876337\n",
       "50%      105814.872141\n",
       "75%      106712.703370\n",
       "max      107610.534600\n",
       "dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pipe_rmses = -cross_val_score(new_geo_pipeline,\n",
    "                                  housing.iloc[:5000],\n",
    "                                  housing_labels.iloc[:5000],\n",
    "                                  scoring=\"neg_root_mean_squared_error\",\n",
    "                                  cv=3)\n",
    "pd.Series(new_pipe_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, that's terrible! Apparently the cluster similarity features were much better. But perhaps we should tune the `KNeighborsRegressor`'s hyperparameters? That's what the next exercise is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Automatically explore some preparation options using `RandomSearchCV`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessing',\n",
       "                                              ColumnTransformer(transformers=[('bedrooms',\n",
       "                                                                               Pipeline(steps=[('simpleimputer',\n",
       "                                                                                                SimpleImputer(strategy='median')),\n",
       "                                                                                               ('functiontransformer',\n",
       "                                                                                                FunctionTransformer(feature_names_out=<function ratio_name at 0x1a5b6fd90>,\n",
       "                                                                                                                    func=<function column_ratio at 0x1a5695bd0>)),\n",
       "                                                                                               ('standardscaler',\n",
       "                                                                                                StandardScaler()...\n",
       "                   param_distributions={'preprocessing__geo__estimator__n_neighbors': range(1, 30),\n",
       "                                        'preprocessing__geo__estimator__weights': ['distance',\n",
       "                                                                                   'uniform'],\n",
       "                                        'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a63fda80>,\n",
       "                                        'svr__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a63fe410>},\n",
       "                   random_state=42, scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    \"preprocessing__geo__estimator__n_neighbors\": range(1, 30),\n",
    "    \"preprocessing__geo__estimator__weights\": [\"distance\", \"uniform\"],\n",
    "    \"svr__C\": loguniform(20, 200_000),\n",
    "    \"svr__gamma\": expon(scale=1.0),\n",
    "}\n",
    "\n",
    "new_geo_rnd_search = RandomizedSearchCV(new_geo_pipeline,\n",
    "                                        param_distributions=param_distribs,\n",
    "                                        n_iter=50,\n",
    "                                        cv=3,\n",
    "                                        scoring='neg_root_mean_squared_error',\n",
    "                                        random_state=42)\n",
    "new_geo_rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106775.63787128967"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_geo_rnd_search_rmse = -new_geo_rnd_search.best_score_\n",
    "new_geo_rnd_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well... at least we tried! It looks like the cluster similarity features are definitely better than the KNN feature. But perhaps you could try having both? And maybe training on the full training set would help as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: _Try to implement the `StandardScalerClone` class again from scratch, then add support for the `inverse_transform()` method: executing `scaler.inverse_transform(scaler.fit_transform(X))` should return an array very close to `X`. Then add support for feature names: set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()` method: it should have one optional `input_features=None` argument. If passed, the method should check that its length matches `n_features_in_`, and it should match `feature_names_in_` if it is defined, then `input_features` should be returned. If `input_features` is `None`, then the method should return `feature_names_in_` if it is defined or `np.array([\"x0\", \"x1\", ...])` with length `n_features_in_` otherwise._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X_orig = X\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        if hasattr(X_orig, \"columns\"):\n",
    "            self.feature_names_in_ = np.array(X_orig.columns, dtype=object)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        if self.n_features_in_ != X.shape[1]:\n",
    "            raise ValueError(\"Unexpected number of features\")\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        if self.n_features_in_ != X.shape[1]:\n",
    "            raise ValueError(\"Unexpected number of features\")\n",
    "        X = X * self.scale_\n",
    "        return X + self.mean_ if self.with_mean else X\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            return getattr(self, \"feature_names_in_\",\n",
    "                           [f\"x{i}\" for i in range(self.n_features_in_)])\n",
    "        else:\n",
    "            if len(input_features) != self.n_features_in_:\n",
    "                raise ValueError(\"Invalid number of features\")\n",
    "            if hasattr(self, \"feature_names_in_\") and not np.all(\n",
    "                self.feature_names_in_ == input_features\n",
    "            ):\n",
    "                raise ValueError(\"input_features â‰  feature_names_in_\")\n",
    "            return input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our custom transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    " \n",
    "check_estimator(StandardScalerClone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors, that's a great start, we respect the Scikit-Learn API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ensure the transformation works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 3)\n",
    "\n",
    "scaler = StandardScalerClone()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "assert np.allclose(X_scaled, (X - X.mean(axis=0)) / X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about setting `with_mean=False`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScalerClone(with_mean=False)\n",
    "X_scaled_uncentered = scaler.fit_transform(X)\n",
    "\n",
    "assert np.allclose(X_scaled_uncentered, X / X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And does the inverse work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScalerClone()\n",
    "X_back = scaler.inverse_transform(scaler.fit_transform(X))\n",
    "\n",
    "assert np.allclose(X, X_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the feature names out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(scaler.get_feature_names_out() == [\"x0\", \"x1\", \"x2\"])\n",
    "assert np.all(scaler.get_feature_names_out([\"a\", \"b\", \"c\"]) == [\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we fit a DataFrame, are the feature in and out ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\": np.random.rand(100), \"b\": np.random.rand(100)})\n",
    "scaler = StandardScalerClone()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "assert np.all(scaler.feature_names_in_ == [\"a\", \"b\"])\n",
    "assert np.all(scaler.get_feature_names_out() == [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good! That's all for today! ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You already know quite a lot about Machine Learning. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
